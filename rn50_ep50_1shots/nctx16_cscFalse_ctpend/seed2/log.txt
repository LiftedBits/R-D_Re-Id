***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/rn50_ep50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2
resume: 
root: C:/Users/nawka/OneDrive/Desktop/GNR697/COCO/CoOp/DATA/oxford_flowers
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: C:/Users/nawka/OneDrive/Desktop/GNR697/COCO/CoOp/DATA/oxford_flowers
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.1.2
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Home Single Language
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2050
Nvidia driver version: 546.26
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=1300
DeviceID=CPU0
Family=205
L2CacheSize=6656
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=1300
Name=12th Gen Intel(R) Core(TM) i5-1235U
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.3
[pip3] numpydoc==1.5.0
[pip3] torch==2.1.2
[pip3] torchaudio==2.1.2
[pip3] torchvision==0.16.2
[conda] _anaconda_depends         2023.09             py311_mkl_1  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2023.1.0         h6b88ed4_46357  
[conda] mkl-service               2.4.0           py311h2bbff1b_1  
[conda] mkl_fft                   1.3.8           py311h2bbff1b_0  
[conda] mkl_random                1.2.4           py311h59b6b97_0  
[conda] numpy                     1.24.3          py311hdab7c0b_1  
[conda] numpy-base                1.24.3          py311hd01c5d8_1  
[conda] numpydoc                  1.5.0           py311haa95532_0  
[conda] pytorch                   2.1.2           py3.11_cuda12.1_cudnn8_0    pytorch
[conda] pytorch-cuda              12.1                 hde6ce7c_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     2.1.2                    pypi_0    pypi
[conda] torchaudio                2.1.2                    pypi_0    pypi
[conda] torchvision               0.16.2                   pypi_0    pypi
        Pillow (10.0.1)

Loading trainer: CoOp
Loading dataset: OxfordFlowers
Reading split from C:\Users\nawka\OneDrive\Desktop\GNR697\COCO\CoOp\DATA\oxford_flowers\oxford_flowers\split_zhou_OxfordFlowers.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to C:\Users\nawka\OneDrive\Desktop\GNR697\COCO\CoOp\DATA\oxford_flowers\oxford_flowers\split_fewshot\shot_1-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  102
# val      102
# test     2,463
---------  -------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2\tensorboard)
epoch [1/50] batch [1/3] time 98.893 (98.893) data 95.335 (95.335) loss 4.0273 (4.0273) acc 9.3750 (9.3750) lr 1.0000e-05 eta 4:05:35
epoch [1/50] batch [2/3] time 0.552 (49.723) data 0.001 (47.668) loss 4.3203 (4.1738) acc 9.3750 (9.3750) lr 1.0000e-05 eta 2:02:38
epoch [1/50] batch [3/3] time 0.551 (33.332) data 0.000 (31.779) loss 4.0781 (4.1419) acc 9.3750 (9.3750) lr 2.0000e-03 eta 1:21:39
epoch [2/50] batch [1/3] time 97.208 (97.208) data 96.360 (96.360) loss 3.7031 (3.7031) acc 15.6250 (15.6250) lr 2.0000e-03 eta 3:56:32
epoch [2/50] batch [2/3] time 0.548 (48.878) data 0.001 (48.181) loss 3.9805 (3.8418) acc 31.2500 (23.4375) lr 2.0000e-03 eta 1:58:07
epoch [2/50] batch [3/3] time 0.555 (32.770) data 0.000 (32.120) loss 3.1484 (3.6107) acc 21.8750 (22.9167) lr 1.9980e-03 eta 1:18:38
epoch [3/50] batch [1/3] time 96.196 (96.196) data 95.390 (95.390) loss 2.8145 (2.8145) acc 34.3750 (34.3750) lr 1.9980e-03 eta 3:49:15
epoch [3/50] batch [2/3] time 0.547 (48.372) data 0.000 (47.695) loss 2.4316 (2.6230) acc 53.1250 (43.7500) lr 1.9980e-03 eta 1:54:28
epoch [3/50] batch [3/3] time 0.553 (32.432) data 0.000 (31.797) loss 2.6621 (2.6361) acc 43.7500 (43.7500) lr 1.9921e-03 eta 1:16:12
epoch [4/50] batch [1/3] time 94.991 (94.991) data 94.208 (94.208) loss 2.0840 (2.0840) acc 56.2500 (56.2500) lr 1.9921e-03 eta 3:41:38
epoch [4/50] batch [2/3] time 0.546 (47.769) data 0.000 (47.104) loss 2.3457 (2.2148) acc 50.0000 (53.1250) lr 1.9921e-03 eta 1:50:39
epoch [4/50] batch [3/3] time 0.558 (32.032) data 0.000 (31.403) loss 3.0137 (2.4811) acc 40.6250 (48.9583) lr 1.9823e-03 eta 1:13:40
epoch [5/50] batch [1/3] time 92.423 (92.423) data 91.688 (91.688) loss 1.8281 (1.8281) acc 50.0000 (50.0000) lr 1.9823e-03 eta 3:31:01
epoch [5/50] batch [2/3] time 0.547 (46.485) data 0.000 (45.844) loss 2.2012 (2.0146) acc 43.7500 (46.8750) lr 1.9823e-03 eta 1:45:21
epoch [5/50] batch [3/3] time 0.555 (31.175) data 0.001 (30.563) loss 2.5684 (2.1992) acc 40.6250 (44.7917) lr 1.9686e-03 eta 1:10:08
epoch [6/50] batch [1/3] time 91.775 (91.775) data 90.978 (90.978) loss 2.2285 (2.2285) acc 56.2500 (56.2500) lr 1.9686e-03 eta 3:24:57
epoch [6/50] batch [2/3] time 0.547 (46.161) data 0.000 (45.489) loss 1.9482 (2.0884) acc 50.0000 (53.1250) lr 1.9686e-03 eta 1:42:19
epoch [6/50] batch [3/3] time 0.554 (30.959) data 0.000 (30.326) loss 2.4492 (2.2087) acc 59.3750 (55.2083) lr 1.9511e-03 eta 1:08:06
epoch [7/50] batch [1/3] time 94.096 (94.096) data 93.177 (93.177) loss 2.3027 (2.3027) acc 50.0000 (50.0000) lr 1.9511e-03 eta 3:25:26
epoch [7/50] batch [2/3] time 0.548 (47.322) data 0.000 (46.589) loss 1.6045 (1.9536) acc 68.7500 (59.3750) lr 1.9511e-03 eta 1:42:31
epoch [7/50] batch [3/3] time 0.556 (31.734) data 0.001 (31.059) loss 1.5244 (1.8105) acc 59.3750 (59.3750) lr 1.9298e-03 eta 1:08:13
epoch [8/50] batch [1/3] time 91.298 (91.298) data 90.450 (90.450) loss 1.4697 (1.4697) acc 71.8750 (71.8750) lr 1.9298e-03 eta 3:14:46
epoch [8/50] batch [2/3] time 0.548 (45.923) data 0.001 (45.225) loss 2.3535 (1.9116) acc 43.7500 (57.8125) lr 1.9298e-03 eta 1:37:12
epoch [8/50] batch [3/3] time 0.556 (30.801) data 0.000 (30.150) loss 1.5723 (1.7985) acc 62.5000 (59.3750) lr 1.9048e-03 eta 1:04:40
epoch [9/50] batch [1/3] time 95.518 (95.518) data 94.704 (94.704) loss 1.7939 (1.7939) acc 62.5000 (62.5000) lr 1.9048e-03 eta 3:18:59
epoch [9/50] batch [2/3] time 0.549 (48.034) data 0.000 (47.352) loss 1.7373 (1.7656) acc 56.2500 (59.3750) lr 1.9048e-03 eta 1:39:16
epoch [9/50] batch [3/3] time 0.556 (32.208) data 0.000 (31.568) loss 1.7451 (1.7588) acc 59.3750 (59.3750) lr 1.8763e-03 eta 1:06:01
epoch [10/50] batch [1/3] time 103.481 (103.481) data 102.684 (102.684) loss 1.5557 (1.5557) acc 59.3750 (59.3750) lr 1.8763e-03 eta 3:30:24
epoch [10/50] batch [2/3] time 0.550 (52.015) data 0.000 (51.342) loss 1.0547 (1.3052) acc 75.0000 (67.1875) lr 1.8763e-03 eta 1:44:53
epoch [10/50] batch [3/3] time 0.556 (34.862) data 0.000 (34.228) loss 2.5723 (1.7275) acc 40.6250 (58.3333) lr 1.8443e-03 eta 1:09:43
epoch [11/50] batch [1/3] time 99.844 (99.844) data 98.974 (98.974) loss 2.0625 (2.0625) acc 56.2500 (56.2500) lr 1.8443e-03 eta 3:18:01
epoch [11/50] batch [2/3] time 0.548 (50.196) data 0.000 (49.487) loss 1.2285 (1.6455) acc 65.6250 (60.9375) lr 1.8443e-03 eta 1:38:43
epoch [11/50] batch [3/3] time 0.555 (33.649) data 0.001 (32.992) loss 1.1846 (1.4919) acc 75.0000 (65.6250) lr 1.8090e-03 eta 1:05:36
epoch [12/50] batch [1/3] time 114.876 (114.876) data 114.025 (114.025) loss 1.2881 (1.2881) acc 71.8750 (71.8750) lr 1.8090e-03 eta 3:42:05
epoch [12/50] batch [2/3] time 0.551 (57.714) data 0.000 (57.013) loss 1.3623 (1.3252) acc 56.2500 (64.0625) lr 1.8090e-03 eta 1:50:37
epoch [12/50] batch [3/3] time 0.557 (38.661) data 0.000 (38.008) loss 1.9141 (1.5215) acc 43.7500 (57.2917) lr 1.7705e-03 eta 1:13:27
epoch [13/50] batch [1/3] time 122.434 (122.434) data 121.686 (121.686) loss 1.4824 (1.4824) acc 62.5000 (62.5000) lr 1.7705e-03 eta 3:50:34
epoch [13/50] batch [2/3] time 0.548 (61.491) data 0.001 (60.843) loss 1.8018 (1.6421) acc 59.3750 (60.9375) lr 1.7705e-03 eta 1:54:46
epoch [13/50] batch [3/3] time 0.556 (41.179) data 0.000 (40.562) loss 1.6367 (1.6403) acc 59.3750 (60.4167) lr 1.7290e-03 eta 1:16:10
epoch [14/50] batch [1/3] time 97.891 (97.891) data 97.087 (97.087) loss 1.8291 (1.8291) acc 59.3750 (59.3750) lr 1.7290e-03 eta 2:59:28
epoch [14/50] batch [2/3] time 0.548 (49.220) data 0.000 (48.543) loss 1.3594 (1.5942) acc 71.8750 (65.6250) lr 1.7290e-03 eta 1:29:24
epoch [14/50] batch [3/3] time 0.556 (32.998) data 0.001 (32.362) loss 1.5283 (1.5723) acc 65.6250 (65.6250) lr 1.6845e-03 eta 0:59:23
epoch [15/50] batch [1/3] time 95.501 (95.501) data 94.692 (94.692) loss 1.8281 (1.8281) acc 53.1250 (53.1250) lr 1.6845e-03 eta 2:50:18
epoch [15/50] batch [2/3] time 0.547 (48.024) data 0.000 (47.346) loss 1.4893 (1.6587) acc 62.5000 (57.8125) lr 1.6845e-03 eta 1:24:50
epoch [15/50] batch [3/3] time 0.555 (32.201) data 0.000 (31.564) loss 1.7305 (1.6826) acc 46.8750 (54.1667) lr 1.6374e-03 eta 0:56:21
epoch [16/50] batch [1/3] time 94.732 (94.732) data 93.880 (93.880) loss 1.2754 (1.2754) acc 75.0000 (75.0000) lr 1.6374e-03 eta 2:44:12
epoch [16/50] batch [2/3] time 0.547 (47.639) data 0.000 (46.940) loss 2.0527 (1.6641) acc 43.7500 (59.3750) lr 1.6374e-03 eta 1:21:46
epoch [16/50] batch [3/3] time 0.556 (31.945) data 0.000 (31.293) loss 1.1943 (1.5075) acc 65.6250 (61.4583) lr 1.5878e-03 eta 0:54:18
epoch [17/50] batch [1/3] time 94.789 (94.789) data 93.880 (93.880) loss 1.3096 (1.3096) acc 68.7500 (68.7500) lr 1.5878e-03 eta 2:39:33
epoch [17/50] batch [2/3] time 0.547 (47.668) data 0.000 (46.940) loss 1.3320 (1.3208) acc 71.8750 (70.3125) lr 1.5878e-03 eta 1:19:26
epoch [17/50] batch [3/3] time 0.556 (31.964) data 0.001 (31.294) loss 1.3105 (1.3174) acc 65.6250 (68.7500) lr 1.5358e-03 eta 0:52:44
epoch [18/50] batch [1/3] time 96.171 (96.171) data 95.263 (95.263) loss 1.5830 (1.5830) acc 65.6250 (65.6250) lr 1.5358e-03 eta 2:37:04
epoch [18/50] batch [2/3] time 0.548 (48.359) data 0.000 (47.632) loss 1.1963 (1.3896) acc 71.8750 (68.7500) lr 1.5358e-03 eta 1:18:10
epoch [18/50] batch [3/3] time 0.557 (32.425) data 0.000 (31.754) loss 0.9302 (1.2365) acc 65.6250 (67.7083) lr 1.4818e-03 eta 0:51:52
epoch [19/50] batch [1/3] time 94.551 (94.551) data 93.800 (93.800) loss 1.5029 (1.5029) acc 71.8750 (71.8750) lr 1.4818e-03 eta 2:29:42
epoch [19/50] batch [2/3] time 0.549 (47.550) data 0.000 (46.900) loss 1.3213 (1.4121) acc 65.6250 (68.7500) lr 1.4818e-03 eta 1:14:29
epoch [19/50] batch [3/3] time 0.556 (31.885) data 0.000 (31.267) loss 1.4717 (1.4320) acc 59.3750 (65.6250) lr 1.4258e-03 eta 0:49:25
epoch [20/50] batch [1/3] time 99.166 (99.166) data 98.423 (98.423) loss 0.9551 (0.9551) acc 81.2500 (81.2500) lr 1.4258e-03 eta 2:32:03
epoch [20/50] batch [2/3] time 0.549 (49.857) data 0.000 (49.212) loss 1.9658 (1.4604) acc 43.7500 (62.5000) lr 1.4258e-03 eta 1:15:37
epoch [20/50] batch [3/3] time 0.557 (33.424) data 0.001 (32.808) loss 1.1152 (1.3454) acc 71.8750 (65.6250) lr 1.3681e-03 eta 0:50:08
epoch [21/50] batch [1/3] time 94.747 (94.747) data 93.997 (93.997) loss 1.3047 (1.3047) acc 65.6250 (65.6250) lr 1.3681e-03 eta 2:20:32
epoch [21/50] batch [2/3] time 0.548 (47.647) data 0.000 (46.998) loss 1.7207 (1.5127) acc 59.3750 (62.5000) lr 1.3681e-03 eta 1:09:52
epoch [21/50] batch [3/3] time 0.555 (31.950) data 0.000 (31.332) loss 1.1963 (1.4072) acc 62.5000 (62.5000) lr 1.3090e-03 eta 0:46:19
epoch [22/50] batch [1/3] time 98.902 (98.902) data 98.052 (98.052) loss 1.3984 (1.3984) acc 62.5000 (62.5000) lr 1.3090e-03 eta 2:21:45
epoch [22/50] batch [2/3] time 0.548 (49.725) data 0.000 (49.026) loss 0.8101 (1.1042) acc 87.5000 (75.0000) lr 1.3090e-03 eta 1:10:26
epoch [22/50] batch [3/3] time 0.554 (33.335) data 0.000 (32.684) loss 1.4404 (1.2163) acc 59.3750 (69.7917) lr 1.2487e-03 eta 0:46:40
epoch [23/50] batch [1/3] time 95.591 (95.591) data 94.747 (94.747) loss 1.2852 (1.2852) acc 68.7500 (68.7500) lr 1.2487e-03 eta 2:12:14
epoch [23/50] batch [2/3] time 0.547 (48.069) data 0.000 (47.373) loss 1.1396 (1.2124) acc 78.1250 (73.4375) lr 1.2487e-03 eta 1:05:41
epoch [23/50] batch [3/3] time 0.556 (32.231) data 0.000 (31.582) loss 1.3662 (1.2637) acc 68.7500 (71.8750) lr 1.1874e-03 eta 0:43:30
epoch [24/50] batch [1/3] time 95.944 (95.944) data 95.113 (95.113) loss 1.1904 (1.1904) acc 68.7500 (68.7500) lr 1.1874e-03 eta 2:07:55
epoch [24/50] batch [2/3] time 0.547 (48.245) data 0.000 (47.557) loss 1.6729 (1.4316) acc 62.5000 (65.6250) lr 1.1874e-03 eta 1:03:31
epoch [24/50] batch [3/3] time 0.556 (32.349) data 0.000 (31.704) loss 0.8140 (1.2257) acc 78.1250 (69.7917) lr 1.1253e-03 eta 0:42:03
epoch [25/50] batch [1/3] time 94.916 (94.916) data 94.080 (94.080) loss 1.0586 (1.0586) acc 71.8750 (71.8750) lr 1.1253e-03 eta 2:01:48
epoch [25/50] batch [2/3] time 0.547 (47.732) data 0.000 (47.040) loss 1.8037 (1.4312) acc 59.3750 (65.6250) lr 1.1253e-03 eta 1:00:27
epoch [25/50] batch [3/3] time 0.555 (32.006) data 0.001 (31.360) loss 0.9775 (1.2799) acc 75.0000 (68.7500) lr 1.0628e-03 eta 0:40:00
epoch [26/50] batch [1/3] time 94.752 (94.752) data 93.904 (93.904) loss 1.2305 (1.2305) acc 75.0000 (75.0000) lr 1.0628e-03 eta 1:56:51
epoch [26/50] batch [2/3] time 0.547 (47.649) data 0.000 (46.952) loss 0.8633 (1.0469) acc 81.2500 (78.1250) lr 1.0628e-03 eta 0:57:58
epoch [26/50] batch [3/3] time 0.555 (31.951) data 0.000 (31.301) loss 1.2666 (1.1201) acc 71.8750 (76.0417) lr 1.0000e-03 eta 0:38:20
epoch [27/50] batch [1/3] time 97.364 (97.364) data 96.541 (96.541) loss 1.2129 (1.2129) acc 71.8750 (71.8750) lr 1.0000e-03 eta 1:55:12
epoch [27/50] batch [2/3] time 0.548 (48.956) data 0.000 (48.271) loss 1.5244 (1.3687) acc 62.5000 (67.1875) lr 1.0000e-03 eta 0:57:06
epoch [27/50] batch [3/3] time 0.555 (32.822) data 0.000 (32.180) loss 1.5332 (1.4235) acc 65.6250 (66.6667) lr 9.3721e-04 eta 0:37:44
epoch [28/50] batch [1/3] time 95.849 (95.849) data 94.982 (94.982) loss 1.1270 (1.1270) acc 75.0000 (75.0000) lr 9.3721e-04 eta 1:48:37
epoch [28/50] batch [2/3] time 0.547 (48.198) data 0.000 (47.491) loss 1.1045 (1.1157) acc 71.8750 (73.4375) lr 9.3721e-04 eta 0:53:49
epoch [28/50] batch [3/3] time 0.557 (32.318) data 0.001 (31.661) loss 1.3271 (1.1862) acc 65.6250 (70.8333) lr 8.7467e-04 eta 0:35:32
epoch [29/50] batch [1/3] time 95.947 (95.947) data 95.094 (95.094) loss 1.0977 (1.0977) acc 71.8750 (71.8750) lr 8.7467e-04 eta 1:43:56
epoch [29/50] batch [2/3] time 0.550 (48.248) data 0.001 (47.548) loss 1.5254 (1.3115) acc 59.3750 (65.6250) lr 8.7467e-04 eta 0:51:27
epoch [29/50] batch [3/3] time 0.557 (32.351) data 0.000 (31.698) loss 1.2041 (1.2757) acc 68.7500 (66.6667) lr 8.1262e-04 eta 0:33:58
epoch [30/50] batch [1/3] time 95.756 (95.756) data 94.948 (94.948) loss 1.4082 (1.4082) acc 65.6250 (65.6250) lr 8.1262e-04 eta 1:38:56
epoch [30/50] batch [2/3] time 0.548 (48.152) data 0.000 (47.474) loss 1.1211 (1.2646) acc 71.8750 (68.7500) lr 8.1262e-04 eta 0:48:57
epoch [30/50] batch [3/3] time 0.556 (32.287) data 0.000 (31.649) loss 1.4932 (1.3408) acc 68.7500 (68.7500) lr 7.5131e-04 eta 0:32:17
epoch [31/50] batch [1/3] time 97.117 (97.117) data 96.280 (96.280) loss 1.5254 (1.5254) acc 65.6250 (65.6250) lr 7.5131e-04 eta 1:35:29
epoch [31/50] batch [2/3] time 0.549 (48.833) data 0.000 (48.140) loss 1.2520 (1.3887) acc 65.6250 (65.6250) lr 7.5131e-04 eta 0:47:12
epoch [31/50] batch [3/3] time 0.557 (32.741) data 0.001 (32.094) loss 0.5098 (1.0957) acc 87.5000 (72.9167) lr 6.9098e-04 eta 0:31:06
epoch [32/50] batch [1/3] time 97.538 (97.538) data 96.696 (96.696) loss 1.6650 (1.6650) acc 53.1250 (53.1250) lr 6.9098e-04 eta 1:31:02
epoch [32/50] batch [2/3] time 0.547 (49.042) data 0.000 (48.348) loss 0.9067 (1.2859) acc 81.2500 (67.1875) lr 6.9098e-04 eta 0:44:57
epoch [32/50] batch [3/3] time 0.555 (32.880) data 0.000 (32.232) loss 1.0977 (1.2231) acc 71.8750 (68.7500) lr 6.3188e-04 eta 0:29:35
epoch [33/50] batch [1/3] time 95.099 (95.099) data 94.318 (94.318) loss 1.3467 (1.3467) acc 62.5000 (62.5000) lr 6.3188e-04 eta 1:24:00
epoch [33/50] batch [2/3] time 0.549 (47.824) data 0.000 (47.159) loss 0.9409 (1.1438) acc 75.0000 (68.7500) lr 6.3188e-04 eta 0:41:26
epoch [33/50] batch [3/3] time 0.556 (32.068) data 0.000 (31.439) loss 0.9688 (1.0854) acc 75.0000 (70.8333) lr 5.7422e-04 eta 0:27:15
epoch [34/50] batch [1/3] time 93.777 (93.777) data 92.894 (92.894) loss 1.4619 (1.4619) acc 53.1250 (53.1250) lr 5.7422e-04 eta 1:18:08
epoch [34/50] batch [2/3] time 0.547 (47.162) data 0.000 (46.447) loss 0.9771 (1.2195) acc 75.0000 (64.0625) lr 5.7422e-04 eta 0:38:30
epoch [34/50] batch [3/3] time 0.554 (31.626) data 0.000 (30.965) loss 1.2109 (1.2166) acc 71.8750 (66.6667) lr 5.1825e-04 eta 0:25:18
epoch [35/50] batch [1/3] time 94.242 (94.242) data 93.450 (93.450) loss 1.3623 (1.3623) acc 68.7500 (68.7500) lr 5.1825e-04 eta 1:13:49
epoch [35/50] batch [2/3] time 0.548 (47.395) data 0.001 (46.725) loss 0.9014 (1.1318) acc 65.6250 (67.1875) lr 5.1825e-04 eta 0:36:20
epoch [35/50] batch [3/3] time 0.555 (31.782) data 0.000 (31.150) loss 1.6602 (1.3079) acc 59.3750 (64.5833) lr 4.6417e-04 eta 0:23:50
epoch [36/50] batch [1/3] time 95.388 (95.388) data 94.641 (94.641) loss 1.3486 (1.3486) acc 59.3750 (59.3750) lr 4.6417e-04 eta 1:09:57
epoch [36/50] batch [2/3] time 0.547 (47.967) data 0.000 (47.320) loss 0.8574 (1.1030) acc 75.0000 (67.1875) lr 4.6417e-04 eta 0:34:22
epoch [36/50] batch [3/3] time 0.556 (32.164) data 0.000 (31.547) loss 1.1172 (1.1077) acc 78.1250 (70.8333) lr 4.1221e-04 eta 0:22:30
epoch [37/50] batch [1/3] time 94.557 (94.557) data 93.812 (93.812) loss 1.0430 (1.0430) acc 75.0000 (75.0000) lr 4.1221e-04 eta 1:04:36
epoch [37/50] batch [2/3] time 0.548 (47.552) data 0.000 (46.906) loss 1.0439 (1.0435) acc 68.7500 (71.8750) lr 4.1221e-04 eta 0:31:42
epoch [37/50] batch [3/3] time 0.555 (31.887) data 0.001 (31.271) loss 1.0605 (1.0492) acc 62.5000 (68.7500) lr 3.6258e-04 eta 0:20:43
epoch [38/50] batch [1/3] time 92.765 (92.765) data 91.862 (91.862) loss 1.0059 (1.0059) acc 68.7500 (68.7500) lr 3.6258e-04 eta 0:58:45
epoch [38/50] batch [2/3] time 0.549 (46.657) data 0.001 (45.932) loss 1.3115 (1.1587) acc 65.6250 (67.1875) lr 3.6258e-04 eta 0:28:46
epoch [38/50] batch [3/3] time 0.554 (31.289) data 0.000 (30.621) loss 0.6099 (0.9757) acc 84.3750 (72.9167) lr 3.1545e-04 eta 0:18:46
epoch [39/50] batch [1/3] time 93.256 (93.256) data 92.518 (92.518) loss 1.0479 (1.0479) acc 78.1250 (78.1250) lr 3.1545e-04 eta 0:54:23
epoch [39/50] batch [2/3] time 0.548 (46.902) data 0.000 (46.259) loss 1.0635 (1.0557) acc 71.8750 (75.0000) lr 3.1545e-04 eta 0:26:34
epoch [39/50] batch [3/3] time 0.556 (31.453) data 0.001 (30.840) loss 0.8691 (0.9935) acc 71.8750 (73.9583) lr 2.7103e-04 eta 0:17:17
epoch [40/50] batch [1/3] time 94.929 (94.929) data 94.108 (94.108) loss 1.3271 (1.3271) acc 62.5000 (62.5000) lr 2.7103e-04 eta 0:50:37
epoch [40/50] batch [2/3] time 0.548 (47.739) data 0.000 (47.054) loss 1.3887 (1.3579) acc 62.5000 (62.5000) lr 2.7103e-04 eta 0:24:39
epoch [40/50] batch [3/3] time 0.556 (32.011) data 0.000 (31.369) loss 0.7974 (1.1711) acc 84.3750 (69.7917) lr 2.2949e-04 eta 0:16:00
epoch [41/50] batch [1/3] time 91.897 (91.897) data 91.079 (91.079) loss 0.9805 (0.9805) acc 78.1250 (78.1250) lr 2.2949e-04 eta 0:44:25
epoch [41/50] batch [2/3] time 0.548 (46.222) data 0.000 (45.539) loss 1.2236 (1.1021) acc 71.8750 (75.0000) lr 2.2949e-04 eta 0:21:34
epoch [41/50] batch [3/3] time 0.556 (31.000) data 0.000 (30.360) loss 0.9175 (1.0405) acc 78.1250 (76.0417) lr 1.9098e-04 eta 0:13:57
epoch [42/50] batch [1/3] time 90.989 (90.989) data 90.077 (90.077) loss 1.3115 (1.3115) acc 59.3750 (59.3750) lr 1.9098e-04 eta 0:39:25
epoch [42/50] batch [2/3] time 0.549 (45.769) data 0.000 (45.039) loss 0.8896 (1.1006) acc 78.1250 (68.7500) lr 1.9098e-04 eta 0:19:04
epoch [42/50] batch [3/3] time 0.556 (30.698) data 0.000 (30.026) loss 1.3125 (1.1712) acc 65.6250 (67.7083) lr 1.5567e-04 eta 0:12:16
epoch [43/50] batch [1/3] time 95.686 (95.686) data 94.780 (94.780) loss 1.0498 (1.0498) acc 75.0000 (75.0000) lr 1.5567e-04 eta 0:36:40
epoch [43/50] batch [2/3] time 0.548 (48.117) data 0.000 (47.390) loss 1.2705 (1.1602) acc 59.3750 (67.1875) lr 1.5567e-04 eta 0:17:38
epoch [43/50] batch [3/3] time 0.556 (32.263) data 0.000 (31.593) loss 0.8472 (1.0558) acc 81.2500 (71.8750) lr 1.2369e-04 eta 0:11:17
epoch [44/50] batch [1/3] time 89.239 (89.239) data 88.437 (88.437) loss 0.9341 (0.9341) acc 75.0000 (75.0000) lr 1.2369e-04 eta 0:29:44
epoch [44/50] batch [2/3] time 0.549 (44.894) data 0.000 (44.218) loss 1.4238 (1.1790) acc 59.3750 (67.1875) lr 1.2369e-04 eta 0:14:12
epoch [44/50] batch [3/3] time 0.557 (30.115) data 0.000 (29.479) loss 0.8252 (1.0610) acc 78.1250 (70.8333) lr 9.5173e-05 eta 0:09:02
epoch [45/50] batch [1/3] time 92.391 (92.391) data 91.645 (91.645) loss 1.2363 (1.2363) acc 65.6250 (65.6250) lr 9.5173e-05 eta 0:26:10
epoch [45/50] batch [2/3] time 0.549 (46.470) data 0.001 (45.823) loss 0.9766 (1.1064) acc 78.1250 (71.8750) lr 9.5173e-05 eta 0:12:23
epoch [45/50] batch [3/3] time 0.556 (31.165) data 0.001 (30.549) loss 1.1719 (1.1283) acc 78.1250 (73.9583) lr 7.0224e-05 eta 0:07:47
epoch [46/50] batch [1/3] time 96.932 (96.932) data 96.108 (96.108) loss 0.9111 (0.9111) acc 81.2500 (81.2500) lr 7.0224e-05 eta 0:22:37
epoch [46/50] batch [2/3] time 0.548 (48.740) data 0.000 (48.054) loss 1.2578 (1.0845) acc 65.6250 (73.4375) lr 7.0224e-05 eta 0:10:33
epoch [46/50] batch [3/3] time 0.557 (32.679) data 0.000 (32.036) loss 0.8325 (1.0005) acc 84.3750 (77.0833) lr 4.8943e-05 eta 0:06:32
epoch [47/50] batch [1/3] time 98.088 (98.088) data 97.162 (97.162) loss 1.1250 (1.1250) acc 68.7500 (68.7500) lr 4.8943e-05 eta 0:17:58
epoch [47/50] batch [2/3] time 0.548 (49.318) data 0.000 (48.581) loss 0.9814 (1.0532) acc 75.0000 (71.8750) lr 4.8943e-05 eta 0:08:13
epoch [47/50] batch [3/3] time 0.556 (33.064) data 0.001 (32.388) loss 1.3242 (1.1436) acc 68.7500 (70.8333) lr 3.1417e-05 eta 0:04:57
epoch [48/50] batch [1/3] time 97.704 (97.704) data 96.776 (96.776) loss 1.2422 (1.2422) acc 68.7500 (68.7500) lr 3.1417e-05 eta 0:13:01
epoch [48/50] batch [2/3] time 0.548 (49.126) data 0.000 (48.388) loss 0.7251 (0.9836) acc 87.5000 (78.1250) lr 3.1417e-05 eta 0:05:43
epoch [48/50] batch [3/3] time 0.556 (32.936) data 0.000 (32.259) loss 1.0068 (0.9914) acc 75.0000 (77.0833) lr 1.7713e-05 eta 0:03:17
epoch [49/50] batch [1/3] time 94.053 (94.053) data 93.283 (93.283) loss 1.0488 (1.0488) acc 68.7500 (68.7500) lr 1.7713e-05 eta 0:07:50
epoch [49/50] batch [2/3] time 0.549 (47.301) data 0.000 (46.642) loss 0.8975 (0.9731) acc 78.1250 (73.4375) lr 1.7713e-05 eta 0:03:09
epoch [49/50] batch [3/3] time 0.556 (31.719) data 0.001 (31.095) loss 0.8970 (0.9478) acc 71.8750 (72.9167) lr 7.8853e-06 eta 0:01:35
epoch [50/50] batch [1/3] time 89.236 (89.236) data 88.454 (88.454) loss 1.3779 (1.3779) acc 56.2500 (56.2500) lr 7.8853e-06 eta 0:02:58
epoch [50/50] batch [2/3] time 0.549 (44.892) data 0.000 (44.227) loss 1.2871 (1.3325) acc 75.0000 (65.6250) lr 7.8853e-06 eta 0:00:44
epoch [50/50] batch [3/3] time 0.557 (30.114) data 0.000 (29.485) loss 0.9336 (1.1995) acc 81.2500 (70.8333) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2\prompt_learner\model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,463
* correct: 1,710
* accuracy: 69.4%
* error: 30.6%
* macro_f1: 65.5%
Elapsed: 1:25:00
