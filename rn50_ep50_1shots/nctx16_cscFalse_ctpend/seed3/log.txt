***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/rn50_ep50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']
output_dir: output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3
resume: 
root: C:/Users/nawka/OneDrive/Desktop/GNR697/COCO/CoOp/DATA/oxford_flowers
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: C:/Users/nawka/OneDrive/Desktop/GNR697/COCO/CoOp/DATA/oxford_flowers
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.1.2
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Home Single Language
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2050
Nvidia driver version: 546.26
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=1300
DeviceID=CPU0
Family=205
L2CacheSize=6656
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=1300
Name=12th Gen Intel(R) Core(TM) i5-1235U
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.3
[pip3] numpydoc==1.5.0
[pip3] torch==2.1.2
[pip3] torchaudio==2.1.2
[pip3] torchvision==0.16.2
[conda] _anaconda_depends         2023.09             py311_mkl_1  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2023.1.0         h6b88ed4_46357  
[conda] mkl-service               2.4.0           py311h2bbff1b_1  
[conda] mkl_fft                   1.3.8           py311h2bbff1b_0  
[conda] mkl_random                1.2.4           py311h59b6b97_0  
[conda] numpy                     1.24.3          py311hdab7c0b_1  
[conda] numpy-base                1.24.3          py311hd01c5d8_1  
[conda] numpydoc                  1.5.0           py311haa95532_0  
[conda] pytorch                   2.1.2           py3.11_cuda12.1_cudnn8_0    pytorch
[conda] pytorch-cuda              12.1                 hde6ce7c_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     2.1.2                    pypi_0    pypi
[conda] torchaudio                2.1.2                    pypi_0    pypi
[conda] torchvision               0.16.2                   pypi_0    pypi
        Pillow (10.0.1)

Loading trainer: CoOp
Loading dataset: OxfordFlowers
Reading split from C:\Users\nawka\OneDrive\Desktop\GNR697\COCO\CoOp\DATA\oxford_flowers\oxford_flowers\split_zhou_OxfordFlowers.json
Creating a 1-shot dataset
Creating a 1-shot dataset
Saving preprocessed few-shot data to C:\Users\nawka\OneDrive\Desktop\GNR697\COCO\CoOp\DATA\oxford_flowers\oxford_flowers\split_fewshot\shot_1-seed_3.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  102
# val      102
# test     2,463
---------  -------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3\tensorboard)
epoch [1/50] batch [1/3] time 95.643 (95.643) data 92.913 (92.913) loss 4.7617 (4.7617) acc 12.5000 (12.5000) lr 1.0000e-05 eta 3:57:30
epoch [1/50] batch [2/3] time 0.551 (48.097) data 0.000 (46.456) loss 4.3164 (4.5391) acc 15.6250 (14.0625) lr 1.0000e-05 eta 1:58:38
epoch [1/50] batch [3/3] time 0.558 (32.251) data 0.001 (30.971) loss 4.2070 (4.4284) acc 12.5000 (13.5417) lr 2.0000e-03 eta 1:19:00
epoch [2/50] batch [1/3] time 97.502 (97.502) data 96.688 (96.688) loss 4.1875 (4.1875) acc 9.3750 (9.3750) lr 2.0000e-03 eta 3:57:15
epoch [2/50] batch [2/3] time 0.547 (49.025) data 0.000 (48.344) loss 3.0176 (3.6025) acc 34.3750 (21.8750) lr 2.0000e-03 eta 1:58:28
epoch [2/50] batch [3/3] time 0.555 (32.868) data 0.000 (32.229) loss 4.6836 (3.9629) acc 9.3750 (17.7083) lr 1.9980e-03 eta 1:18:52
epoch [3/50] batch [1/3] time 96.674 (96.674) data 95.812 (95.812) loss 3.1387 (3.1387) acc 28.1250 (28.1250) lr 1.9980e-03 eta 3:50:24
epoch [3/50] batch [2/3] time 0.547 (48.610) data 0.000 (47.906) loss 2.5508 (2.8447) acc 40.6250 (34.3750) lr 1.9980e-03 eta 1:55:02
epoch [3/50] batch [3/3] time 0.554 (32.591) data 0.000 (31.937) loss 2.2129 (2.6341) acc 46.8750 (38.5417) lr 1.9921e-03 eta 1:16:35
epoch [4/50] batch [1/3] time 90.066 (90.066) data 89.267 (89.267) loss 2.5176 (2.5176) acc 43.7500 (43.7500) lr 1.9921e-03 eta 3:30:09
epoch [4/50] batch [2/3] time 0.549 (45.307) data 0.000 (44.634) loss 3.1602 (2.8389) acc 25.0000 (34.3750) lr 1.9921e-03 eta 1:44:57
epoch [4/50] batch [3/3] time 0.555 (30.390) data 0.000 (29.756) loss 1.4521 (2.3766) acc 65.6250 (44.7917) lr 1.9823e-03 eta 1:09:53
epoch [5/50] batch [1/3] time 91.676 (91.676) data 90.853 (90.853) loss 2.0449 (2.0449) acc 50.0000 (50.0000) lr 1.9823e-03 eta 3:29:19
epoch [5/50] batch [2/3] time 0.546 (46.111) data 0.000 (45.427) loss 2.3457 (2.1953) acc 43.7500 (46.8750) lr 1.9823e-03 eta 1:44:31
epoch [5/50] batch [3/3] time 0.554 (30.926) data 0.000 (30.284) loss 2.6016 (2.3307) acc 43.7500 (45.8333) lr 1.9686e-03 eta 1:09:34
epoch [6/50] batch [1/3] time 90.694 (90.694) data 89.833 (89.833) loss 2.2871 (2.2871) acc 50.0000 (50.0000) lr 1.9686e-03 eta 3:22:33
epoch [6/50] batch [2/3] time 0.546 (45.620) data 0.000 (44.916) loss 2.4512 (2.3691) acc 34.3750 (42.1875) lr 1.9686e-03 eta 1:41:07
epoch [6/50] batch [3/3] time 0.555 (30.599) data 0.000 (29.944) loss 2.0410 (2.2598) acc 56.2500 (46.8750) lr 1.9511e-03 eta 1:07:19
epoch [7/50] batch [1/3] time 88.803 (88.803) data 87.958 (87.958) loss 1.7910 (1.7910) acc 56.2500 (56.2500) lr 1.9511e-03 eta 3:13:53
epoch [7/50] batch [2/3] time 0.548 (44.675) data 0.000 (43.979) loss 1.9668 (1.8789) acc 50.0000 (53.1250) lr 1.9511e-03 eta 1:36:47
epoch [7/50] batch [3/3] time 0.556 (29.969) data 0.000 (29.319) loss 2.0605 (1.9395) acc 43.7500 (50.0000) lr 1.9298e-03 eta 1:04:25
epoch [8/50] batch [1/3] time 90.907 (90.907) data 89.986 (89.986) loss 2.0293 (2.0293) acc 46.8750 (46.8750) lr 1.9298e-03 eta 3:13:56
epoch [8/50] batch [2/3] time 0.548 (45.727) data 0.001 (44.994) loss 2.0684 (2.0488) acc 56.2500 (51.5625) lr 1.9298e-03 eta 1:36:47
epoch [8/50] batch [3/3] time 0.556 (30.670) data 0.000 (29.996) loss 1.8105 (1.9694) acc 56.2500 (53.1250) lr 1.9048e-03 eta 1:04:24
epoch [9/50] batch [1/3] time 89.826 (89.826) data 88.914 (88.914) loss 1.6338 (1.6338) acc 56.2500 (56.2500) lr 1.9048e-03 eta 3:07:08
epoch [9/50] batch [2/3] time 0.547 (45.187) data 0.001 (44.457) loss 1.8770 (1.7554) acc 56.2500 (56.2500) lr 1.9048e-03 eta 1:33:23
epoch [9/50] batch [3/3] time 0.555 (30.309) data 0.000 (29.638) loss 2.1738 (1.8949) acc 53.1250 (55.2083) lr 1.8763e-03 eta 1:02:08
epoch [10/50] batch [1/3] time 90.363 (90.363) data 89.567 (89.567) loss 1.8281 (1.8281) acc 56.2500 (56.2500) lr 1.8763e-03 eta 3:03:44
epoch [10/50] batch [2/3] time 0.549 (45.456) data 0.000 (44.783) loss 1.8984 (1.8633) acc 56.2500 (56.2500) lr 1.8763e-03 eta 1:31:40
epoch [10/50] batch [3/3] time 0.555 (30.489) data 0.000 (29.856) loss 2.2793 (2.0020) acc 43.7500 (52.0833) lr 1.8443e-03 eta 1:00:58
epoch [11/50] batch [1/3] time 88.965 (88.965) data 88.079 (88.079) loss 1.8359 (1.8359) acc 53.1250 (53.1250) lr 1.8443e-03 eta 2:56:26
epoch [11/50] batch [2/3] time 0.546 (44.755) data 0.000 (44.040) loss 1.4180 (1.6270) acc 68.7500 (60.9375) lr 1.8443e-03 eta 1:28:01
epoch [11/50] batch [3/3] time 0.554 (30.022) data 0.000 (29.360) loss 1.8945 (1.7161) acc 50.0000 (57.2917) lr 1.8090e-03 eta 0:58:32
epoch [12/50] batch [1/3] time 88.413 (88.413) data 87.594 (87.594) loss 1.9551 (1.9551) acc 50.0000 (50.0000) lr 1.8090e-03 eta 2:50:55
epoch [12/50] batch [2/3] time 0.549 (44.481) data 0.000 (43.797) loss 1.6279 (1.7915) acc 53.1250 (51.5625) lr 1.8090e-03 eta 1:25:15
epoch [12/50] batch [3/3] time 0.554 (29.839) data 0.000 (29.198) loss 2.1836 (1.9222) acc 59.3750 (54.1667) lr 1.7705e-03 eta 0:56:41
epoch [13/50] batch [1/3] time 89.664 (89.664) data 88.818 (88.818) loss 1.8516 (1.8516) acc 50.0000 (50.0000) lr 1.7705e-03 eta 2:48:51
epoch [13/50] batch [2/3] time 0.547 (45.105) data 0.000 (44.409) loss 2.0781 (1.9648) acc 43.7500 (46.8750) lr 1.7705e-03 eta 1:24:11
epoch [13/50] batch [3/3] time 0.554 (30.255) data 0.000 (29.606) loss 2.0020 (1.9772) acc 50.0000 (47.9167) lr 1.7290e-03 eta 0:55:58
epoch [14/50] batch [1/3] time 91.387 (91.387) data 90.540 (90.540) loss 1.9922 (1.9922) acc 43.7500 (43.7500) lr 1.7290e-03 eta 2:47:32
epoch [14/50] batch [2/3] time 0.547 (45.967) data 0.000 (45.270) loss 1.7002 (1.8462) acc 59.3750 (51.5625) lr 1.7290e-03 eta 1:23:30
epoch [14/50] batch [3/3] time 0.555 (30.829) data 0.000 (30.180) loss 1.6172 (1.7699) acc 62.5000 (55.2083) lr 1.6845e-03 eta 0:55:29
epoch [15/50] batch [1/3] time 88.268 (88.268) data 87.425 (87.425) loss 1.6826 (1.6826) acc 62.5000 (62.5000) lr 1.6845e-03 eta 2:37:24
epoch [15/50] batch [2/3] time 0.547 (44.408) data 0.000 (43.712) loss 1.8750 (1.7788) acc 56.2500 (59.3750) lr 1.6845e-03 eta 1:18:27
epoch [15/50] batch [3/3] time 0.554 (29.790) data 0.000 (29.142) loss 1.8438 (1.8005) acc 50.0000 (56.2500) lr 1.6374e-03 eta 0:52:07
epoch [16/50] batch [1/3] time 89.361 (89.361) data 88.623 (88.623) loss 1.7188 (1.7188) acc 59.3750 (59.3750) lr 1.6374e-03 eta 2:34:53
epoch [16/50] batch [2/3] time 0.548 (44.954) data 0.000 (44.312) loss 1.2627 (1.4907) acc 75.0000 (67.1875) lr 1.6374e-03 eta 1:17:10
epoch [16/50] batch [3/3] time 0.554 (30.154) data 0.000 (29.541) loss 1.6230 (1.5348) acc 53.1250 (62.5000) lr 1.5878e-03 eta 0:51:15
epoch [17/50] batch [1/3] time 89.347 (89.347) data 88.548 (88.548) loss 1.4014 (1.4014) acc 68.7500 (68.7500) lr 1.5878e-03 eta 2:30:24
epoch [17/50] batch [2/3] time 0.547 (44.947) data 0.000 (44.274) loss 1.9287 (1.6650) acc 50.0000 (59.3750) lr 1.5878e-03 eta 1:14:54
epoch [17/50] batch [3/3] time 0.555 (30.150) data 0.000 (29.516) loss 1.4453 (1.5918) acc 62.5000 (60.4167) lr 1.5358e-03 eta 0:49:44
epoch [18/50] batch [1/3] time 88.968 (88.968) data 88.058 (88.058) loss 1.7793 (1.7793) acc 56.2500 (56.2500) lr 1.5358e-03 eta 2:25:18
epoch [18/50] batch [2/3] time 0.547 (44.758) data 0.000 (44.029) loss 1.9609 (1.8701) acc 56.2500 (56.2500) lr 1.5358e-03 eta 1:12:21
epoch [18/50] batch [3/3] time 0.554 (30.023) data 0.000 (29.353) loss 1.3994 (1.7132) acc 59.3750 (57.2917) lr 1.4818e-03 eta 0:48:02
epoch [19/50] batch [1/3] time 89.540 (89.540) data 88.802 (88.802) loss 1.6826 (1.6826) acc 56.2500 (56.2500) lr 1.4818e-03 eta 2:21:46
epoch [19/50] batch [2/3] time 0.549 (45.044) data 0.000 (44.401) loss 1.5449 (1.6138) acc 56.2500 (56.2500) lr 1.4818e-03 eta 1:10:34
epoch [19/50] batch [3/3] time 0.556 (30.215) data 0.000 (29.601) loss 1.8887 (1.7054) acc 53.1250 (55.2083) lr 1.4258e-03 eta 0:46:49
epoch [20/50] batch [1/3] time 88.897 (88.897) data 88.152 (88.152) loss 1.6680 (1.6680) acc 46.8750 (46.8750) lr 1.4258e-03 eta 2:16:18
epoch [20/50] batch [2/3] time 0.547 (44.722) data 0.000 (44.076) loss 1.6982 (1.6831) acc 56.2500 (51.5625) lr 1.4258e-03 eta 1:07:49
epoch [20/50] batch [3/3] time 0.555 (30.000) data 0.000 (29.384) loss 1.7812 (1.7158) acc 56.2500 (53.1250) lr 1.3681e-03 eta 0:44:59
epoch [21/50] batch [1/3] time 88.590 (88.590) data 87.840 (87.840) loss 1.4268 (1.4268) acc 68.7500 (68.7500) lr 1.3681e-03 eta 2:11:24
epoch [21/50] batch [2/3] time 0.546 (44.568) data 0.000 (43.920) loss 2.0371 (1.7319) acc 56.2500 (62.5000) lr 1.3681e-03 eta 1:05:21
epoch [21/50] batch [3/3] time 0.555 (29.897) data 0.000 (29.280) loss 1.5107 (1.6582) acc 62.5000 (62.5000) lr 1.3090e-03 eta 0:43:21
epoch [22/50] batch [1/3] time 89.168 (89.168) data 88.417 (88.417) loss 1.7227 (1.7227) acc 53.1250 (53.1250) lr 1.3090e-03 eta 2:07:48
epoch [22/50] batch [2/3] time 0.547 (44.858) data 0.000 (44.209) loss 1.3652 (1.5439) acc 68.7500 (60.9375) lr 1.3090e-03 eta 1:03:32
epoch [22/50] batch [3/3] time 0.555 (30.090) data 0.001 (29.473) loss 1.3926 (1.4935) acc 71.8750 (64.5833) lr 1.2487e-03 eta 0:42:07
epoch [23/50] batch [1/3] time 89.173 (89.173) data 88.326 (88.326) loss 1.5205 (1.5205) acc 65.6250 (65.6250) lr 1.2487e-03 eta 2:03:21
epoch [23/50] batch [2/3] time 0.546 (44.860) data 0.000 (44.163) loss 1.4287 (1.4746) acc 56.2500 (60.9375) lr 1.2487e-03 eta 1:01:18
epoch [23/50] batch [3/3] time 0.555 (30.092) data 0.000 (29.442) loss 1.4092 (1.4528) acc 62.5000 (61.4583) lr 1.1874e-03 eta 0:40:37
epoch [24/50] batch [1/3] time 90.949 (90.949) data 90.142 (90.142) loss 1.6836 (1.6836) acc 62.5000 (62.5000) lr 1.1874e-03 eta 2:01:15
epoch [24/50] batch [2/3] time 0.547 (45.748) data 0.000 (45.071) loss 1.5693 (1.6265) acc 46.8750 (54.6875) lr 1.1874e-03 eta 1:00:14
epoch [24/50] batch [3/3] time 0.554 (30.684) data 0.000 (30.047) loss 1.6523 (1.6351) acc 65.6250 (58.3333) lr 1.1253e-03 eta 0:39:53
epoch [25/50] batch [1/3] time 88.542 (88.542) data 87.732 (87.732) loss 1.2412 (1.2412) acc 71.8750 (71.8750) lr 1.1253e-03 eta 1:53:37
epoch [25/50] batch [2/3] time 0.547 (44.545) data 0.000 (43.866) loss 1.2705 (1.2559) acc 71.8750 (71.8750) lr 1.1253e-03 eta 0:56:25
epoch [25/50] batch [3/3] time 0.555 (29.881) data 0.000 (29.244) loss 2.1309 (1.5475) acc 46.8750 (63.5417) lr 1.0628e-03 eta 0:37:21
epoch [26/50] batch [1/3] time 88.786 (88.786) data 87.985 (87.985) loss 1.6699 (1.6699) acc 62.5000 (62.5000) lr 1.0628e-03 eta 1:49:30
epoch [26/50] batch [2/3] time 0.549 (44.667) data 0.000 (43.992) loss 1.4766 (1.5732) acc 68.7500 (65.6250) lr 1.0628e-03 eta 0:54:20
epoch [26/50] batch [3/3] time 0.556 (29.964) data 0.000 (29.328) loss 1.1865 (1.4443) acc 65.6250 (65.6250) lr 1.0000e-03 eta 0:35:57
epoch [27/50] batch [1/3] time 89.191 (89.191) data 88.357 (88.357) loss 1.1211 (1.1211) acc 65.6250 (65.6250) lr 1.0000e-03 eta 1:45:32
epoch [27/50] batch [2/3] time 0.549 (44.870) data 0.000 (44.179) loss 1.8057 (1.4634) acc 56.2500 (60.9375) lr 1.0000e-03 eta 0:52:20
epoch [27/50] batch [3/3] time 0.555 (30.098) data 0.001 (29.453) loss 1.9336 (1.6201) acc 56.2500 (59.3750) lr 9.3721e-04 eta 0:34:36
epoch [28/50] batch [1/3] time 88.738 (88.738) data 87.990 (87.990) loss 1.4277 (1.4277) acc 43.7500 (43.7500) lr 9.3721e-04 eta 1:40:34
epoch [28/50] batch [2/3] time 0.548 (44.643) data 0.000 (43.995) loss 1.6953 (1.5615) acc 56.2500 (50.0000) lr 9.3721e-04 eta 0:49:51
epoch [28/50] batch [3/3] time 0.556 (29.948) data 0.000 (29.330) loss 1.3096 (1.4775) acc 59.3750 (53.1250) lr 8.7467e-04 eta 0:32:56
epoch [29/50] batch [1/3] time 89.566 (89.566) data 88.679 (88.679) loss 1.6572 (1.6572) acc 50.0000 (50.0000) lr 8.7467e-04 eta 1:37:01
epoch [29/50] batch [2/3] time 0.549 (45.057) data 0.001 (44.340) loss 1.4395 (1.5483) acc 59.3750 (54.6875) lr 8.7467e-04 eta 0:48:03
epoch [29/50] batch [3/3] time 0.557 (30.224) data 0.000 (29.560) loss 1.4404 (1.5124) acc 62.5000 (57.2917) lr 8.1262e-04 eta 0:31:44
epoch [30/50] batch [1/3] time 88.141 (88.141) data 87.295 (87.295) loss 1.3623 (1.3623) acc 68.7500 (68.7500) lr 8.1262e-04 eta 1:31:04
epoch [30/50] batch [2/3] time 0.548 (44.345) data 0.000 (43.648) loss 1.7246 (1.5435) acc 56.2500 (62.5000) lr 8.1262e-04 eta 0:45:05
epoch [30/50] batch [3/3] time 0.555 (29.748) data 0.000 (29.098) loss 1.4326 (1.5065) acc 65.6250 (63.5417) lr 7.5131e-04 eta 0:29:44
epoch [31/50] batch [1/3] time 88.542 (88.542) data 87.791 (87.791) loss 1.2490 (1.2490) acc 71.8750 (71.8750) lr 7.5131e-04 eta 1:27:04
epoch [31/50] batch [2/3] time 0.548 (44.545) data 0.000 (43.896) loss 1.1572 (1.2031) acc 68.7500 (70.3125) lr 7.5131e-04 eta 0:43:03
epoch [31/50] batch [3/3] time 0.555 (29.882) data 0.000 (29.264) loss 1.9570 (1.4544) acc 43.7500 (61.4583) lr 6.9098e-04 eta 0:28:23
epoch [32/50] batch [1/3] time 89.084 (89.084) data 88.165 (88.165) loss 1.4424 (1.4424) acc 65.6250 (65.6250) lr 6.9098e-04 eta 1:23:08
epoch [32/50] batch [2/3] time 0.548 (44.816) data 0.000 (44.082) loss 1.3311 (1.3867) acc 65.6250 (65.6250) lr 6.9098e-04 eta 0:41:04
epoch [32/50] batch [3/3] time 0.555 (30.062) data 0.000 (29.388) loss 1.6904 (1.4880) acc 59.3750 (63.5417) lr 6.3188e-04 eta 0:27:03
epoch [33/50] batch [1/3] time 89.408 (89.408) data 88.558 (88.558) loss 1.5498 (1.5498) acc 56.2500 (56.2500) lr 6.3188e-04 eta 1:18:58
epoch [33/50] batch [2/3] time 0.549 (44.979) data 0.000 (44.279) loss 1.0830 (1.3164) acc 81.2500 (68.7500) lr 6.3188e-04 eta 0:38:58
epoch [33/50] batch [3/3] time 0.559 (30.172) data 0.000 (29.519) loss 1.3262 (1.3197) acc 59.3750 (65.6250) lr 5.7422e-04 eta 0:25:38
epoch [34/50] batch [1/3] time 90.340 (90.340) data 89.494 (89.494) loss 1.3770 (1.3770) acc 65.6250 (65.6250) lr 5.7422e-04 eta 1:15:16
epoch [34/50] batch [2/3] time 0.546 (45.443) data 0.000 (44.747) loss 1.9658 (1.6714) acc 50.0000 (57.8125) lr 5.7422e-04 eta 0:37:06
epoch [34/50] batch [3/3] time 0.555 (30.480) data 0.000 (29.831) loss 1.0654 (1.4694) acc 71.8750 (62.5000) lr 5.1825e-04 eta 0:24:23
epoch [35/50] batch [1/3] time 88.611 (88.611) data 87.862 (87.862) loss 1.9531 (1.9531) acc 53.1250 (53.1250) lr 5.1825e-04 eta 1:09:24
epoch [35/50] batch [2/3] time 0.547 (44.579) data 0.000 (43.931) loss 1.4570 (1.7051) acc 59.3750 (56.2500) lr 5.1825e-04 eta 0:34:10
epoch [35/50] batch [3/3] time 0.555 (29.904) data 0.000 (29.287) loss 1.1738 (1.5280) acc 65.6250 (59.3750) lr 4.6417e-04 eta 0:22:25
epoch [36/50] batch [1/3] time 90.495 (90.495) data 89.645 (89.645) loss 1.5508 (1.5508) acc 59.3750 (59.3750) lr 4.6417e-04 eta 1:06:21
epoch [36/50] batch [2/3] time 0.547 (45.521) data 0.000 (44.823) loss 1.2734 (1.4121) acc 71.8750 (65.6250) lr 4.6417e-04 eta 0:32:37
epoch [36/50] batch [3/3] time 0.556 (30.533) data 0.001 (29.882) loss 1.4248 (1.4163) acc 68.7500 (66.6667) lr 4.1221e-04 eta 0:21:22
epoch [37/50] batch [1/3] time 90.080 (90.080) data 89.330 (89.330) loss 1.3340 (1.3340) acc 65.6250 (65.6250) lr 4.1221e-04 eta 1:01:33
epoch [37/50] batch [2/3] time 0.547 (45.314) data 0.000 (44.665) loss 1.6855 (1.5098) acc 53.1250 (59.3750) lr 4.1221e-04 eta 0:30:12
epoch [37/50] batch [3/3] time 0.554 (30.394) data 0.000 (29.777) loss 1.5830 (1.5342) acc 62.5000 (60.4167) lr 3.6258e-04 eta 0:19:45
epoch [38/50] batch [1/3] time 88.156 (88.156) data 87.416 (87.416) loss 1.4639 (1.4639) acc 65.6250 (65.6250) lr 3.6258e-04 eta 0:55:49
epoch [38/50] batch [2/3] time 0.547 (44.352) data 0.000 (43.708) loss 1.3828 (1.4233) acc 71.8750 (68.7500) lr 3.6258e-04 eta 0:27:21
epoch [38/50] batch [3/3] time 0.556 (29.753) data 0.001 (29.139) loss 1.3770 (1.4079) acc 62.5000 (66.6667) lr 3.1545e-04 eta 0:17:51
epoch [39/50] batch [1/3] time 90.332 (90.332) data 89.485 (89.485) loss 1.1094 (1.1094) acc 71.8750 (71.8750) lr 3.1545e-04 eta 0:52:41
epoch [39/50] batch [2/3] time 0.547 (45.439) data 0.000 (44.742) loss 1.4941 (1.3018) acc 68.7500 (70.3125) lr 3.1545e-04 eta 0:25:44
epoch [39/50] batch [3/3] time 0.556 (30.478) data 0.000 (29.828) loss 1.7383 (1.4473) acc 53.1250 (64.5833) lr 2.7103e-04 eta 0:16:45
epoch [40/50] batch [1/3] time 88.348 (88.348) data 87.433 (87.433) loss 1.4941 (1.4941) acc 71.8750 (71.8750) lr 2.7103e-04 eta 0:47:07
epoch [40/50] batch [2/3] time 0.547 (44.448) data 0.000 (43.716) loss 1.7422 (1.6182) acc 56.2500 (64.0625) lr 2.7103e-04 eta 0:22:57
epoch [40/50] batch [3/3] time 0.555 (29.817) data 0.000 (29.144) loss 1.0117 (1.4160) acc 78.1250 (68.7500) lr 2.2949e-04 eta 0:14:54
epoch [41/50] batch [1/3] time 88.346 (88.346) data 87.536 (87.536) loss 1.3652 (1.3652) acc 59.3750 (59.3750) lr 2.2949e-04 eta 0:42:42
epoch [41/50] batch [2/3] time 0.549 (44.447) data 0.000 (43.768) loss 1.4658 (1.4155) acc 62.5000 (60.9375) lr 2.2949e-04 eta 0:20:44
epoch [41/50] batch [3/3] time 0.555 (29.817) data 0.000 (29.179) loss 1.5703 (1.4671) acc 75.0000 (65.6250) lr 1.9098e-04 eta 0:13:25
epoch [42/50] batch [1/3] time 89.320 (89.320) data 88.408 (88.408) loss 1.5039 (1.5039) acc 56.2500 (56.2500) lr 1.9098e-04 eta 0:38:42
epoch [42/50] batch [2/3] time 0.548 (44.934) data 0.000 (44.204) loss 1.4805 (1.4922) acc 59.3750 (57.8125) lr 1.9098e-04 eta 0:18:43
epoch [42/50] batch [3/3] time 0.555 (30.141) data 0.000 (29.469) loss 1.1465 (1.3770) acc 75.0000 (63.5417) lr 1.5567e-04 eta 0:12:03
epoch [43/50] batch [1/3] time 91.981 (91.981) data 91.218 (91.218) loss 1.6475 (1.6475) acc 68.7500 (68.7500) lr 1.5567e-04 eta 0:35:15
epoch [43/50] batch [2/3] time 0.549 (46.265) data 0.000 (45.609) loss 1.5146 (1.5811) acc 65.6250 (67.1875) lr 1.5567e-04 eta 0:16:57
epoch [43/50] batch [3/3] time 0.555 (31.028) data 0.000 (30.406) loss 1.4580 (1.5400) acc 65.6250 (66.6667) lr 1.2369e-04 eta 0:10:51
epoch [44/50] batch [1/3] time 89.318 (89.318) data 88.512 (88.512) loss 1.1484 (1.1484) acc 68.7500 (68.7500) lr 1.2369e-04 eta 0:29:46
epoch [44/50] batch [2/3] time 0.548 (44.933) data 0.000 (44.256) loss 1.7744 (1.4614) acc 59.3750 (64.0625) lr 1.2369e-04 eta 0:14:13
epoch [44/50] batch [3/3] time 0.556 (30.141) data 0.000 (29.504) loss 1.6787 (1.5339) acc 56.2500 (61.4583) lr 9.5173e-05 eta 0:09:02
epoch [45/50] batch [1/3] time 88.884 (88.884) data 88.019 (88.019) loss 1.4688 (1.4688) acc 56.2500 (56.2500) lr 9.5173e-05 eta 0:25:11
epoch [45/50] batch [2/3] time 0.548 (44.716) data 0.000 (44.010) loss 1.2783 (1.3735) acc 75.0000 (65.6250) lr 9.5173e-05 eta 0:11:55
epoch [45/50] batch [3/3] time 0.556 (29.996) data 0.001 (29.340) loss 1.7598 (1.5023) acc 65.6250 (65.6250) lr 7.0224e-05 eta 0:07:29
epoch [46/50] batch [1/3] time 88.403 (88.403) data 87.506 (87.506) loss 1.1387 (1.1387) acc 75.0000 (75.0000) lr 7.0224e-05 eta 0:20:37
epoch [46/50] batch [2/3] time 0.548 (44.475) data 0.000 (43.753) loss 1.4365 (1.2876) acc 62.5000 (68.7500) lr 7.0224e-05 eta 0:09:38
epoch [46/50] batch [3/3] time 0.555 (29.835) data 0.000 (29.169) loss 1.0859 (1.2204) acc 71.8750 (69.7917) lr 4.8943e-05 eta 0:05:58
epoch [47/50] batch [1/3] time 89.349 (89.349) data 88.598 (88.598) loss 0.7275 (0.7275) acc 81.2500 (81.2500) lr 4.8943e-05 eta 0:16:22
epoch [47/50] batch [2/3] time 0.548 (44.948) data 0.000 (44.299) loss 1.6846 (1.2061) acc 50.0000 (65.6250) lr 4.8943e-05 eta 0:07:29
epoch [47/50] batch [3/3] time 0.555 (30.150) data 0.000 (29.533) loss 1.6113 (1.3411) acc 50.0000 (60.4167) lr 3.1417e-05 eta 0:04:31
epoch [48/50] batch [1/3] time 90.135 (90.135) data 89.373 (89.373) loss 1.5615 (1.5615) acc 56.2500 (56.2500) lr 3.1417e-05 eta 0:12:01
epoch [48/50] batch [2/3] time 0.549 (45.342) data 0.000 (44.687) loss 1.2734 (1.4175) acc 68.7500 (62.5000) lr 3.1417e-05 eta 0:05:17
epoch [48/50] batch [3/3] time 0.556 (30.413) data 0.000 (29.791) loss 1.5947 (1.4766) acc 65.6250 (63.5417) lr 1.7713e-05 eta 0:03:02
epoch [49/50] batch [1/3] time 89.103 (89.103) data 88.309 (88.309) loss 1.5430 (1.5430) acc 56.2500 (56.2500) lr 1.7713e-05 eta 0:07:25
epoch [49/50] batch [2/3] time 0.548 (44.826) data 0.000 (44.155) loss 0.9771 (1.2600) acc 78.1250 (67.1875) lr 1.7713e-05 eta 0:02:59
epoch [49/50] batch [3/3] time 0.555 (30.069) data 0.000 (29.436) loss 1.4014 (1.3071) acc 65.6250 (66.6667) lr 7.8853e-06 eta 0:01:30
epoch [50/50] batch [1/3] time 88.758 (88.758) data 87.859 (87.859) loss 1.7764 (1.7764) acc 59.3750 (59.3750) lr 7.8853e-06 eta 0:02:57
epoch [50/50] batch [2/3] time 0.548 (44.653) data 0.000 (43.929) loss 1.0059 (1.3911) acc 78.1250 (68.7500) lr 7.8853e-06 eta 0:00:44
epoch [50/50] batch [3/3] time 0.555 (29.954) data 0.001 (29.287) loss 1.5352 (1.4391) acc 53.1250 (63.5417) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3\prompt_learner\model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,463
* correct: 1,664
* accuracy: 67.6%
* error: 32.4%
* macro_f1: 62.7%
Elapsed: 1:19:31
