{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccaab21-69ff-4ef0-bb87-1d811c988da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\nawka\\appdata\\local\\temp\\pip-req-build-zdmf5woc\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from clip==1.0) (6.1.3)\n",
      "Requirement already satisfied: regex in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from clip==1.0) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from clip==1.0) (0.16.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (10.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369579 sha256=b6c10c4999738366868f0d02d426853b1d3d10f15a6b65708f01cec1d651ab5f\n",
      "  Stored in directory: C:\\Users\\nawka\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ig3t4z7c\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\nawka\\AppData\\Local\\Temp\\pip-req-build-zdmf5woc'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e80a27-5a23-4bbe-9832-28af50988e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fcedcd-b058-4c0c-8657-49dd12646660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [01:07<00:00, 5.27MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22d5b569-f4d5-4d5c-80ed-931ca5d5dfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Conditional Operator (CoOp) function\n",
    "def coop(features1, features2):\n",
    "    # Concatenate the features along the last dimension\n",
    "    concatenated_features = tf.concat([features1, features2], axis=-1)\n",
    "    return concatenated_features\n",
    "\n",
    "# Define a simple Siamese network with CoOp\n",
    "def siamese_network(image):\n",
    "    input_shape = image.shape[1:]\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    features = clip_model.encode_image(image)\n",
    "\n",
    "    # Split the features\n",
    "    features1, features2 = tf.split(features, 2, axis=0)\n",
    "\n",
    "    # Apply CoOp\n",
    "    combined_features = coop(features1, features2)\n",
    "\n",
    "    # Fully connected layer for classification\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(combined_features)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3351de56-74be-4398-b8a3-b8a5a0a17626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b12981c-fa71-4099-a359-ddd4efc25f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46c2293-0cc7-465e-b6af-44c8715f55e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_image_as_np_array(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    img_array = np.array(img)\n",
    "    return img_array\n",
    "\n",
    "# Example file paths for demonstration purposes\n",
    "file_path1 = files[5]\n",
    "file_path2 = files[1]\n",
    "\n",
    "# Get numpy arrays for the images\n",
    "image1_array = get_image_as_np_array(file_path1)\n",
    "image2_array = get_image_as_np_array(file_path2)\n",
    "\n",
    "# Now you have numpy arrays for both images\n",
    "# You can perform any desired operations with these arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f876e08a-cadb-45dc-8da2-6fce55685a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m img2_preprocessed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img2_preprocessed, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create Siamese network model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m siamese_model \u001b[38;5;241m=\u001b[39m siamese_network(img1_preprocessed)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m, in \u001b[0;36msiamese_network\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      9\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m---> 11\u001b[0m features \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mencode_image(image)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Split the features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m features1, features2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(features, \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\clip\\model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual(image\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:261\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    257\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124m    tf.experimental.numpy.experimental_enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assuming you have images stored as numpy arrays in img1 and img2\n",
    "# img1 = image1_array  # Load image 1\n",
    "# img2 = image2_array  # Load image 2\n",
    "\n",
    "img1 = Image.open(files[5])  # Load image 1\n",
    "img2 = Image.open(files[1])  # Load image 2\n",
    "\n",
    "# Preprocess images using CLIP preprocess function\n",
    "img1_preprocessed = clip_preprocess(img1)\n",
    "img2_preprocessed = clip_preprocess(img2)\n",
    "\n",
    "# img1_preprocessed = get_image_as_np_array(img1_preprocessed)\n",
    "# img2_preprocessed = get_image_as_np_array(img2_preprocessed)\n",
    "\n",
    "# Expand dimensions to match CLIP input shape (batch_size, height, width, channels)\n",
    "img1_preprocessed = tf.expand_dims(img1_preprocessed, axis=0)\n",
    "img2_preprocessed = tf.expand_dims(img2_preprocessed, axis=0)\n",
    "\n",
    "# Create Siamese network model\n",
    "siamese_model = siamese_network(img1_preprocessed)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with your dataset\n",
    "# Assuming you have a dataset with pairs of images and their labels (0 for different persons, 1 for same person)\n",
    "# x_train contains pairs of preprocessed images\n",
    "# y_train contains corresponding labels\n",
    "siamese_model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73353be8-09a6-433a-8dba-48cb1f6ce272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 224, 224), dtype=float32, numpy=\n",
       "array([[[[1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361],\n",
       "         [1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361],\n",
       "         [1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361],\n",
       "         ...,\n",
       "         [1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361],\n",
       "         [1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361],\n",
       "         [1.9303361, 1.9303361, 1.9303361, ..., 1.9303361, 1.9303361,\n",
       "          1.9303361]],\n",
       "\n",
       "        [[2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837],\n",
       "         [2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837],\n",
       "         [2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837],\n",
       "         ...,\n",
       "         [2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837],\n",
       "         [2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837],\n",
       "         [2.0748837, 2.0748837, 2.0748837, ..., 2.0748837, 2.0748837,\n",
       "          2.0748837]],\n",
       "\n",
       "        [[2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ],\n",
       "         [2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ],\n",
       "         [2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ],\n",
       "         ...,\n",
       "         [2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ],\n",
       "         [2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ],\n",
       "         [2.145897 , 2.145897 , 2.145897 , ..., 2.145897 , 2.145897 ,\n",
       "          2.145897 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac80b9-95ac-42d3-bbcd-541449dab758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
