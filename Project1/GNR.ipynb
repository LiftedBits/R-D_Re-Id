{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111794af-b461-4717-9fb7-438e75675ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nawka\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load pre-trained CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e40dd1c-e79e-4d57-aeee-da43cbe4af16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.listdir(\"./\")\n",
    "files = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb0e39f-4a0d-4e5b-b6fb-50911bb1e58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d118190-34cb-4e0f-931e-6c17127711fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__BroadcastTo_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,1,6,1] vs. [1,6,224,224] [Op:BroadcastTo]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(text_feature)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# similarity = tf.matmul(image_feature, tf.reshape(text_feature, (1,1,1,6)))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m reshaped_text_feature \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(text_feature, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Add dimensions\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m reshaped_text_feature \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mbroadcast_to(reshaped_text_feature, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))  \u001b[38;5;66;03m# Broadcast\u001b[39;00m\n\u001b[0;32m     33\u001b[0m similarity \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(image_feature, reshaped_text_feature, transpose_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# similarity = tf.matmul(image_feature, tf.expand_dims(text_feature, axis=-1))  # Expand dims to make text feature compatible for matmul\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:891\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(input, shape, name)\u001b[0m\n\u001b[0;32m    889\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m--> 891\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m broadcast_to_eager_fallback(\n\u001b[0;32m    892\u001b[0m       \u001b[38;5;28minput\u001b[39m, shape, name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    894\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:937\u001b[0m, in \u001b[0;36mbroadcast_to_eager_fallback\u001b[1;34m(input, shape, name, ctx)\u001b[0m\n\u001b[0;32m    935\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, shape]\n\u001b[0;32m    936\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTidx\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tidx)\n\u001b[1;32m--> 937\u001b[0m _result \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastTo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m=\u001b[39m_inputs_flat,\n\u001b[0;32m    938\u001b[0m                            attrs\u001b[38;5;241m=\u001b[39m_attrs, ctx\u001b[38;5;241m=\u001b[39mctx, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    940\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m    941\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastTo\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__BroadcastTo_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,1,6,1] vs. [1,6,224,224] [Op:BroadcastTo]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example image and text pairs\n",
    "image_paths = [files[4], files[1]]\n",
    "text_descriptions = [\"person wearing red shirt\", \"person with backpack\"]\n",
    "\n",
    "# Encode images\n",
    "image_features = []\n",
    "for image_path in image_paths:\n",
    "    image_norm = tf.io.read_file(image_path)\n",
    "    image_norm = tf.image.decode_jpeg(image_norm, channels=3)\n",
    "    image_norm = tf.image.resize(image_norm, (224, 224))  # Resize to CLIP input size\n",
    "    image_norm = tf.cast(image_norm, tf.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    image_norm = clip_processor(images=image_norm, return_tensors=\"tf\")\n",
    "    image_features.append(image_norm)\n",
    "\n",
    "# Encode text descriptions\n",
    "text_features = clip_processor(text_descriptions, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# print(text_features)\n",
    "# Ensure text features have compatible shape\n",
    "# text_features[\"input_ids\"] = tf.tile(text_features[\"input_ids\"], [len(image_paths), 1])  # Repeat text features for each image\n",
    "# print(text_features[\"input_ids\"])\n",
    "# print(image_features)\n",
    "# Compute similarity scores\n",
    "similarity_scores = []\n",
    "for i in range(len(image_paths)):\n",
    "    image_feature = image_features[i][\"pixel_values\"]\n",
    "    text_feature = tf.cast(text_features[\"input_ids\"][i], dtype=image_feature.dtype)  # Cast text feature to the same dtype as image feature\n",
    "    # print(text_feature)\n",
    "    # similarity = tf.matmul(image_feature, tf.reshape(text_feature, (1,1,1,6)))\n",
    "    reshaped_text_feature = tf.reshape(text_feature, (1, 1, 6, 1))  # Add dimensions\n",
    "    reshaped_text_feature = tf.broadcast_to(reshaped_text_feature, (1, 6, 224, 224))  # Broadcast\n",
    "    similarity = tf.matmul(image_feature, reshaped_text_feature, transpose_b=True)\n",
    "    # similarity = tf.matmul(image_feature, tf.expand_dims(text_feature, axis=-1))  # Expand dims to make text feature compatible for matmul\n",
    "    similarity_scores.append(similarity.numpy()[0][0])\n",
    "\n",
    "# Print similarity scores\n",
    "for i, score in enumerate(similarity_scores):\n",
    "    print(f\"Image {i+1} - Text similarity score: {score:.4f}\")\n",
    "\n",
    "# You can now use these similarity scores for person re-identification.\n",
    "# Higher scores indicate better matches between images and text descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f8c634d-4cfd-474c-ba82-7a6601900565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Feature Shape: (1, 3, 224, 224)\n",
      "Text Feature Shape: (1, 6)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute _MklBatchMatMulV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:BatchMatMulV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Ensure text_feature has the expected shape\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m text_feature\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText feature should have shape [1, num_features]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(image_feature, tf\u001b[38;5;241m.\u001b[39mexpand_dims(text_feature, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     17\u001b[0m     similarity_scores\u001b[38;5;241m.\u001b[39mappend(similarity\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Print similarity scores\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute _MklBatchMatMulV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:BatchMatMulV2] name: "
     ]
    }
   ],
   "source": [
    "# Compute similarity scores\n",
    "similarity_scores = []\n",
    "for i in range(len(image_paths)):\n",
    "    image_feature = image_features[i][\"pixel_values\"]\n",
    "    \n",
    "    # Extract a single text feature corresponding to the current image\n",
    "    text_feature = tf.expand_dims(text_features[\"input_ids\"][i], axis=0)\n",
    "    \n",
    "    # Debugging: Print the shapes of image_feature and text_feature\n",
    "    print(\"Image Feature Shape:\", image_feature.shape)\n",
    "    print(\"Text Feature Shape:\", text_feature.shape)\n",
    "    \n",
    "    # Ensure text_feature has the expected shape\n",
    "    assert text_feature.shape[0] == 1, \"Text feature should have shape [1, num_features]\"\n",
    "    \n",
    "    similarity = tf.matmul(image_feature, tf.expand_dims(text_feature, axis=-1))\n",
    "    similarity_scores.append(similarity.numpy()[0][0])\n",
    "\n",
    "# Print similarity scores\n",
    "for i, score in enumerate(similarity_scores):\n",
    "    print(f\"Image {i+1} - Text similarity score: {score:.4f}\")\n",
    "resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d310d5e3-c125-433d-b571-7e23287a6a20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgbeddings in c:\\users\\nawka\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: transformers>=4.17.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from imgbeddings) (4.32.1)\n",
      "Requirement already satisfied: onnxruntime>=1.10.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from imgbeddings) (1.17.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from imgbeddings) (10.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from imgbeddings) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from imgbeddings) (1.3.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (23.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (4.23.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from onnxruntime>=1.10.0->imgbeddings) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from transformers>=4.17.0->imgbeddings) (0.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from tqdm->imgbeddings) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from scikit-learn->imgbeddings) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from scikit-learn->imgbeddings) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from scikit-learn->imgbeddings) (2.2.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.17.0->imgbeddings) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.17.0->imgbeddings) (4.7.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.10.0->imgbeddings) (10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->transformers>=4.17.0->imgbeddings) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->transformers>=4.17.0->imgbeddings) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->transformers>=4.17.0->imgbeddings) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from requests->transformers>=4.17.0->imgbeddings) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.10.0->imgbeddings) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\nawka\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.10.0->imgbeddings) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imgbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a24952-fdf6-4cd6-a823-c82a6162e8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'backpack.jpeg',\n",
       " 'coop_model1.ipynb',\n",
       " 'GNR.ipynb',\n",
       " 'red_shirt_guy.jpg']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "488af742-9e7f-4c61-a78f-acf7f86a7580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nawka\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fd4c7ed0f4428da9e62be8bcd525c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading patch32_v1.onnx:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cf987f43744e92a58919664777f281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nawka\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nawka\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac39882e67b4f9e8c57b12bdd172596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e765d0b812f8496a97c4063796181dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b240f7f51d4a9080da27e0c456f1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5a02c20f184a598f2a705891392dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0abd88c2d047eeb762ac8a858908fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24de68f091924f60bfabbc07438fed15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nawka\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:143: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from imgbeddings import imgbeddings\n",
    "\n",
    "img1 = Image.open(files[4])\n",
    "img2 = Image.open(files[1])\n",
    "ibed = imgbeddings()\n",
    "embeddings = [ibed.to_embeddings(img1), ibed.to_embeddings(img2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2a71050-f8bf-416c-b527-04a720d88a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tf.experimental.numpy.experimental_enable_numpy_behavior()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text_inputs \u001b[38;5;241m=\u001b[39m clip_processor(text_descriptions, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m text_features \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mget_text_features(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_inputs)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compute similarity scores between image embeddings and text features\u001b[39;00m\n\u001b[0;32m      7\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(image_embeddings, text_features, transpose_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1035\u001b[0m, in \u001b[0;36mCLIPModel.get_text_features\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1030\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1031\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1035\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[0;32m   1036\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1037\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1038\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1039\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1040\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1041\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1042\u001b[0m )\n\u001b[0;32m   1044\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m text_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1045\u001b[0m text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:732\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 732\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    733\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not callable"
     ]
    }
   ],
   "source": [
    "# tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "\n",
    "text_inputs = clip_processor(text_descriptions, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "# Compute similarity scores between image embeddings and text features\n",
    "similarity_scores = tf.matmul(image_embeddings, text_features, transpose_b=True)\n",
    "\n",
    "# Get the most similar text description for each image\n",
    "top_matches = tf.argmax(similarity_scores, axis=1)\n",
    "\n",
    "# Print the results\n",
    "for i, match_idx in enumerate(top_matches):\n",
    "    print(f\"Image {i+1}: {text_descriptions[match_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673a4b4-3e14-43df-96ad-2b3f495c2d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
